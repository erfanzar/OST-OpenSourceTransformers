{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[63], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_activation\n",
      "\u001B[1;31mImportError\u001B[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ..modules.activations import get_activation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 32\n",
    "num_heads = 8\n",
    "embedding = 512\n",
    "num_layers = 4\n",
    "embedding % num_heads == 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, c1, c2):\n",
    "        super(Conv, self).__init__()\n",
    "        self.c2 = c2\n",
    "        w = torch.empty(c1, c2)\n",
    "        nn.init.normal_(w, std=0.2)\n",
    "        self.w = nn.Parameter(w)\n",
    "        self.b = nn.Parameter(torch.zeros(c2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_shape = x.size()[:-1] + (self.c2,)\n",
    "        x = torch.addmm(self.b, x.view(-1, x.size(-1)), self.w).view(new_shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([15, 1, 15])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "Conv(5, 15)(torch.ones(15, 1, 5)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def _split_heads(tensor: torch.Tensor):\n",
    "    new_shape = tensor.size()[:-1] + (num_heads, embedding // num_heads)\n",
    "    tensor = tensor.view(new_shape).permute(0, 2, 1, 3)\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def _merge_heads(tensor: torch.Tensor):\n",
    "    tensor = tensor.permute(0, 2, 1, 3)\n",
    "    new_shape = tensor.size()[:-2] + (num_heads * (embedding // num_heads),)\n",
    "    return tensor.view(new_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensor_input = torch.ones(batch, 30, embedding, requires_grad=True)\n",
    "tensor_input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "_split_heads(tensor_input).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "_merge_heads(_split_heads(tensor_input)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    num_embedding: int = 512\n",
    "    num_heads: int = 8\n",
    "    max_len: int = 256\n",
    "    vocab_size: int = 5000\n",
    "    num_layers: int = 2\n",
    "    scale_attn_by_layer_idx: bool = False\n",
    "    use_mask: bool = True\n",
    "    attn_dropout: float = 0.2\n",
    "    residual_dropout: float = 0.2\n",
    "    activation = 'new_gelu'\n",
    "    hidden_size: int = num_embedding\n",
    "    max_position_embeddings = max_len\n",
    "    embd_pdrop: float = 0.1\n",
    "    device:str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    intermediate_size: int = num_embedding * 4\n",
    "\n",
    "\n",
    "class MultiCNNAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super(MultiCNNAttention, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.embedding = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_div = self.embedding // self.num_heads\n",
    "        self.scale_attn_by_layer_idx = config.scale_attn_by_layer_idx\n",
    "        self.use_mask = config.use_mask\n",
    "        if  self.num_heads // self.embedding  != 0:\n",
    "            raise ValueError(\n",
    "                f'hidden_size must be dividable to num_heads {self.num_heads} // {self.embedding} = { self.num_heads // self.embedding}'\n",
    "            )\n",
    "        self.c_attn = Conv(self.embedding, self.embedding * 3)\n",
    "        self.c_proj = Conv(self.embedding, self.embedding)\n",
    "        self.residual_dropout = nn.Dropout(config.residual_dropout)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.register_buffer('bias', torch.tril(\n",
    "            torch.ones(config.max_len, config.max_len, dtype=torch.uint8, device=config.device).view(1, 1,\n",
    "                                                                                                     config.max_len,\n",
    "                                                                                                     config.max_len)))\n",
    "\n",
    "        self.register_buffer('masked_bias', torch.tensor(float(-1e4)))\n",
    "\n",
    "    def _split_heads(self, tensor: torch.Tensor):\n",
    "        new_shape = tensor.size()[:-1] + (self.num_heads, self.num_div)\n",
    "        tensor = tensor.view(new_shape).permute(0, 2, 1, 3)\n",
    "        return tensor\n",
    "\n",
    "    def _merge_heads(self, tensor: torch.Tensor):\n",
    "        tensor = tensor.permute(0, 2, 1, 3)\n",
    "        new_shape = tensor.size()[:-2] + (self.num_heads * self.num_div,)\n",
    "        return tensor.reshape(new_shape)\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask, head_mask):\n",
    "        attn_weight = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        attn_weight = attn_weight / torch.full([], value.size(-1) ** 0.5, dtype=attn_weight.dtype,\n",
    "                                               device=attn_weight.device)\n",
    "        if self.scale_attn_by_layer_idx:\n",
    "            attn_weight /= self.layer_idx\n",
    "        if self.use_mask:\n",
    "            key_len, query_len = key.size(-2), query.size(-2)\n",
    "            masked = self.bias[:, :, key_len - query_len:query_len, :key_len].to(attn_weight.device)\n",
    "            attn_weight = attn_weight.masked_fill(masked == 0, self.masked_bias)\n",
    "        if attention_mask is not None:\n",
    "            attn_weight = attn_weight + attention_mask\n",
    "        attn_weight = nn.functional.softmax(attn_weight, dim=-1)\n",
    "        attn_weight = self.attn_dropout(attn_weight)\n",
    "        attn_weight = attn_weight.type(value.dtype)\n",
    "        if head_mask is not None:\n",
    "            attn_weight = attn_weight * head_mask\n",
    "\n",
    "        attn_weight = torch.matmul(attn_weight, value)\n",
    "        return attn_weight\n",
    "\n",
    "    def forward(self, hidden_state: typing.Optional[torch.Tensor], attention_mask=None, head_mask=None):\n",
    "        query, key, value = self.c_attn(hidden_state).split(self.embedding, dim=2)\n",
    "        query = self._split_heads(query)\n",
    "        key = self._split_heads(key)\n",
    "        value = self._split_heads(value)\n",
    "        attn_output = self._attn(query=query, key=key, value=value, attention_mask=attention_mask, head_mask=head_mask)\n",
    "        attn_output = self.residual_dropout(self.c_proj(self._merge_heads(attn_output)))\n",
    "        return attn_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class PGTMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PGTMLP, self).__init__()\n",
    "        self.c_op = Conv(config.hidden_size, config.intermediate_size)\n",
    "        self.c_proj = Conv(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.residual_dropout)\n",
    "        # self.act = get_activation(config.activation)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = self.c_op(hidden_state)\n",
    "        hidden_state = self.act(hidden_state)\n",
    "        hidden_state = self.c_proj(hidden_state)\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        return hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class PGTBlock(nn.Module):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super(PGTBlock, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.h = MultiCNNAttention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = PGTMLP(config)\n",
    "\n",
    "    def forward(self, hidden_state, attention_mask=None, heads_mask=None):\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.ln1(hidden_state)\n",
    "        hidden_state = self.h(hidden_state, attention_mask, heads_mask) + residual\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.ln2(residual)\n",
    "        hidden_state = self.mlp(hidden_state) + residual\n",
    "        return hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class PGT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([PGTBlock(config, layer_idx=i) for i in range(config.num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.wte = new_embeddings\n",
    "\n",
    "    def forward(self, inputs: typing.Optional[torch.LongTensor], attention_mask=None, heads_mask=None):\n",
    "        token_embeddings = self.wte(inputs)\n",
    "        pos_embeddings = self.wpe(torch.arange(0, inputs.size(-1), dtype=inputs.dtype, device=inputs.device))\n",
    "        hidden = self.drop(token_embeddings + pos_embeddings)\n",
    "        for m in self.h:\n",
    "            hidden = m(hidden, attention_mask=attention_mask, heads_mask=heads_mask)\n",
    "        hidden = self.ln_f(hidden)\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "m = PGT(config=Config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "8.996864"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mm.numel() for mm in m.parameters())/1e6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3,4,5,6,6,6,6,6,8,7,102,562]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "ss = m(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 14, 512])"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}