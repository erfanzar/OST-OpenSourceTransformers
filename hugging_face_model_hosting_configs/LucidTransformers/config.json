{
  "architectures": [
    "LtModelForCausalLM"
  ],
  "auto_map": {
    "AutoConfig": "modelling_LT.LtConfig",
    "AutoModelForCausalLM": "modelling_LT.LtModelForCausalLM"
  },
  "alibi_bias_max": 12,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "max_sequence_length": 2048,
  "num_attention_heads": 16,
  "num_hidden_layers": 16,
  "pad_token_id": 0,
  "softmax_scale": null,
  "transformers_version": "4.28.1",
  "vocab_size": 32003,
  "weight_decay": 0.02,
  "tokenizer_name": "erfanzar/LGeM-7B",
  "torch_dtype": "float16"
}
