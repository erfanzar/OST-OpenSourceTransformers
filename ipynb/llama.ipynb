{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Union\n",
    "import logging\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def broadcast_shaping(x: Optional[torch.Tensor], ferq: Optional[torch.Tensor]):\n",
    "    ndim = x.ndim\n",
    "    logger.debug(f\"x Shape at broadcast_shaping {x.shape}\")\n",
    "    logger.debug(f\"frq Shape at broadcast_shaping {ferq.shape}\")\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert ferq.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return ferq.view(*shape)\n",
    "\n",
    "\n",
    "def rotary_embedding(xq: Optional[torch.Tensor], xk: Optional[torch.Tensor], ferq: Optional[torch.Tensor]):\n",
    "    xq_ = torch.view_as_complex(xq.float().view(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().view(*xk.shape[:-1], -1, 2))\n",
    "    ferq = broadcast_shaping(xq_, ferq)\n",
    "    xq_out = torch.view_as_real(xq_ * ferq).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * ferq).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class LLamaConfig:\n",
    "    eps: Optional[float] = 1e-6\n",
    "    hidden_size: Optional[int] = 680\n",
    "    n_heads: Optional[int] = 12\n",
    "    n_layers: Optional[int] = 8\n",
    "    vocab_size: Optional[int] = 200\n",
    "    max_sentence_length: Optional[int] = 512\n",
    "    max_batch_size: Optional[int] = 32\n",
    "    device: Union[torch.device, str] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "vocab_size: int = 200\n",
    "\n",
    "batch: int = 12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size - 1, size=(batch, 80))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class LLamaAttention(nn.Module):\n",
    "    def __init__(self, config: LLamaConfig):\n",
    "        super(LLamaAttention, self).__init__()\n",
    "        self.local_rank = config.n_heads // 1\n",
    "        self.head_dim = config.hidden_size // config.n_heads\n",
    "        self.wq = nn.Linear(config.hidden_size, config.n_heads * self.head_dim, bias=False,\n",
    "                            )\n",
    "        self.wk = nn.Linear(config.hidden_size, config.n_heads * self.head_dim, bias=False,\n",
    "                            )\n",
    "        self.wv = nn.Linear(config.hidden_size, config.n_heads * self.head_dim, bias=False,\n",
    "                            )\n",
    "        self.wo = nn.Linear(config.n_heads * self.head_dim, config.hidden_size, bias=False,\n",
    "                            )\n",
    "        self.cash_k = torch.zeros(\n",
    "            (config.max_batch_size, config.max_sentence_length, self.local_rank, self.head_dim)).to(config.device)\n",
    "        self.cash_v = torch.zeros(\n",
    "            (config.max_batch_size, config.max_sentence_length, self.local_rank, self.head_dim)).to(config.device)\n",
    "\n",
    "    def forward(self, x: Optional[torch.Tensor], pos_start: int, frq: Optional[torch.Tensor],\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        batch_, seq_len_, _ = x.shape\n",
    "        xq = self.wq(x).view(batch_, seq_len_, self.local_rank, self.head_dim)\n",
    "        xv = self.wv(x).view(batch_, seq_len_, self.local_rank, self.head_dim)\n",
    "        xk = self.wk(x).view(batch_, seq_len_, self.local_rank, self.head_dim)\n",
    "        logger.debug(f'xq : {xq.shape} \\nxv : {xv.shape}\\nxk : {xk.shape}')\n",
    "        # using rotary embedding for key and query\n",
    "        xq, xk = rotary_embedding(xq=xq, xk=xk, ferq=frq)\n",
    "        # we need to cash key and values\n",
    "        self.cash_v = self.cash_v.to(xv)\n",
    "        self.cash_k = self.cash_k.to(xk)\n",
    "        self.cash_k[:batch_, pos_start:pos_start + seq_len_] = xk\n",
    "        self.cash_v[:batch_, pos_start:pos_start + seq_len_] = xq\n",
    "        key = self.cash_k[:batch_, pos_start:pos_start + seq_len_]\n",
    "        value = self.cash_v[:batch_, pos_start:pos_start + seq_len_]\n",
    "        # [batch, seq_len , num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        # [batch, seq_len , num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "        # [batch, seq_len , num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        query = xq.permute(0, 2, 1, 3)\n",
    "        logger.debug(f'key : {key.shape} \\nvalue : {value.shape}\\nquery : {query.shape}')\n",
    "        # key : [batch, num_heads, seq_len, head_dim] -> [batch, seq_len , num_heads, head_dim]\n",
    "        # score : [batch, num_heads, seq_len , head_dim]\n",
    "        attention = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        logger.debug(f'score : {attention.shape}')\n",
    "        if mask is not None:\n",
    "            b, n, s, h = attention.shape\n",
    "            attention += mask[:, :, :s, :h]\n",
    "        attention = nn.functional.softmax(attention, dim=-1)\n",
    "        # after matmul [batch, num_heads, seq_len , head_dim]\n",
    "        comb = torch.matmul(attention, value).permute(0, 2, 1, 3).contiguous().view(batch_, seq_len_, -1)\n",
    "        return self.wo(comb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "config = LLamaConfig()\n",
    "attention = LLamaAttention(config)\n",
    "embedding = nn.Embedding(config.vocab_size, config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "(56, 12)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.head_dim, attention.local_rank"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def precompute_frq_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freq = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freq.device)  # type: ignore\n",
    "    freq = torch.outer(t, freq).float()  # type: ignore\n",
    "    freq = torch.polar(torch.ones_like(freq), freq)  # complex64\n",
    "    return freq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "frq = precompute_frq_cis(attention.head_dim, config.max_sentence_length * 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 80])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "embedded = embedding(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 80, 680])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "start_pos = 0\n",
    "mask = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n          [0., 0., -inf,  ..., -inf, -inf, -inf],\n          [0., 0., 0.,  ..., -inf, -inf, -inf],\n          ...,\n          [0., 0., 0.,  ..., 0., -inf, -inf],\n          [0., 0., 0.,  ..., 0., 0., -inf],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full((1, 1, config.max_sentence_length, config.max_sentence_length), float('-inf'), device=x.device)\n",
    "\n",
    "mask = torch.triu(mask, diagonal=start_pos + 1).type_as(embedded)\n",
    "mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "\n",
    "batch_, seq_len_, _ = embedded.shape\n",
    "chosen_ferq = frq[start_pos:start_pos + seq_len_]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 512, 512])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:xq : torch.Size([12, 80, 12, 56]) \n",
      "xv : torch.Size([12, 80, 12, 56])\n",
      "xk : torch.Size([12, 80, 12, 56])\n",
      "DEBUG:__main__:x Shape at broadcast_shaping torch.Size([12, 80, 12, 28])\n",
      "DEBUG:__main__:frq Shape at broadcast_shaping torch.Size([80, 28])\n",
      "DEBUG:__main__:key : torch.Size([12, 12, 80, 56]) \n",
      "value : torch.Size([12, 12, 80, 56])\n",
      "query : torch.Size([12, 12, 80, 56])\n",
      "DEBUG:__main__:score : torch.Size([12, 12, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "at = attention(embedded, start_pos, chosen_ferq, mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# if you follow your anger you power will answer ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 80, 680])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "emb = torch.randint(1, 10, (1, 8))\n",
    "print(emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "emb = torch.cat([emb, torch.tensor([0, 0, 0, 0, 0, 0, 0, 0]).unsqueeze(0)]).view(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "attention_mask = (emb != 0).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "fka = emb.repeat(10, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "head_mask = torch.triu(torch.full(fka.shape, float('-inf')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., 2., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., 2., 5., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., 2., 5., 7., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., 2., 5., 7., 5., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., 2., 5., 7., 5., 4., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [7., 3., 2., 2., 5., 7., 5., 4., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fka + head_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}