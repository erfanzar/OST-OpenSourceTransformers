{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Union\n",
    "import logging\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def broadcast_shaping(x: Optional[torch.Tensor], ferq: Optional[torch.Tensor]):\n",
    "    ndim = x.ndim\n",
    "    logger.debug(f\"x Shape at broadcast_shaping {x.shape}\")\n",
    "    logger.debug(f\"frq Shape at broadcast_shaping {frq.shape}\")\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert ferq.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return ferq.view(*shape)\n",
    "\n",
    "\n",
    "def rotary_embedding(xq: Optional[torch.Tensor], xk: Optional[torch.Tensor], ferq: Optional[torch.Tensor]):\n",
    "    xq_ = torch.view_as_complex(xq.float().view(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().view(*xk.shape[:-1], -1, 2))\n",
    "    ferq = broadcast_shaping(xq_, ferq)\n",
    "    xq_out = torch.view_as_real(xq_ * ferq).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * ferq).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class LLamaConfig:\n",
    "    eps: Optional[float] = 1e-6\n",
    "    hidden_size: Optional[int] = 680\n",
    "    n_heads: Optional[int] = 12\n",
    "    n_layers: Optional[int] = 8\n",
    "    vocab_size: Optional[int] = 200\n",
    "    max_sentence_length: Optional[int] = 512\n",
    "    max_batch_size: Optional[int] = 32\n",
    "    device: Union[torch.device, str] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "vocab_size: int = 200\n",
    "\n",
    "batch: int = 12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size - 1, size=(batch, 80))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class LLamaAttention(nn.Module):\n",
    "    def __init__(self, config: LLamaConfig):\n",
    "        super(LLamaAttention, self).__init__()\n",
    "        self.local_rank = config.n_heads // 1\n",
    "        self.head_dim = config.hidden_size // config.n_heads\n",
    "        self.wq = nn.Linear(config.hidden_size, config.n_heads * self.head_dim, bias=False,\n",
    "                            )\n",
    "        self.wk = nn.Linear(config.hidden_size, config.n_heads * self.head_dim, bias=False,\n",
    "                            )\n",
    "        self.wv = nn.Linear(config.hidden_size, config.n_heads * self.head_dim, bias=False,\n",
    "                            )\n",
    "        self.wo = nn.Linear(config.n_heads * self.head_dim, config.hidden_size, bias=False,\n",
    "                            )\n",
    "        self.cash_k = torch.zeros(\n",
    "            (config.max_batch_size, config.max_sentence_length, self.local_rank, self.head_dim)).to(config.device)\n",
    "        self.cash_v = torch.zeros(\n",
    "            (config.max_batch_size, config.max_sentence_length, self.local_rank, self.head_dim)).to(config.device)\n",
    "\n",
    "    def forward(self, x: Optional[torch.Tensor], pos_start: int, frq: Optional[torch.Tensor],\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        batch_, seq_len_, _ = x.shape\n",
    "        xq = self.wq(x).view(batch_, seq_len_, self.local_rank, self.head_dim)\n",
    "        xv = self.wv(x).view(batch_, seq_len_, self.local_rank, self.head_dim)\n",
    "        xk = self.wk(x).view(batch_, seq_len_, self.local_rank, self.head_dim)\n",
    "        logger.debug(f'xq : {xq.shape} \\nxv : {xv.shape}\\nxk : {xk.shape}')\n",
    "        # using rotary embedding for key and query\n",
    "        xq, xk = apply_rotary_emb(xq=xq, xk=xk, freqs_cis=frq)\n",
    "        # we need to cash key and values\n",
    "        self.cash_v = self.cash_v.to(xv)\n",
    "        self.cash_k = self.cash_k.to(xk)\n",
    "        self.cash_k[:batch_, pos_start:pos_start + seq_len_] = xk\n",
    "        self.cash_v[:batch_, pos_start:pos_start + seq_len_] = xq\n",
    "        key = self.cash_k[:batch_, pos_start:pos_start + seq_len_]\n",
    "        value = self.cash_v[:batch_, pos_start:pos_start + seq_len_]\n",
    "        # [batch, seq_len , num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        # [batch, seq_len , num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "        # [batch, seq_len , num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        query = xq.permute(0, 2, 1, 3)\n",
    "        logger.debug(f'key : {key.shape} \\nvalue : {value.shape}\\nquery : {query.shape}')\n",
    "        # key : [batch, num_heads, seq_len, head_dim] -> [batch, seq_len , num_heads, head_dim]\n",
    "        # score : [batch, num_heads, seq_len , head_dim]\n",
    "        attention = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        logger.debug(f'score : {attention.shape}')\n",
    "        if mask is not None:\n",
    "            attention += mask\n",
    "        attention = nn.functional.softmax(attention, dim=-1)\n",
    "        # after matmul [batch, num_heads, seq_len , head_dim]\n",
    "        comb = torch.matmul(attention, value).permute(0, 2, 1, 3).contiguous().view(batch_, seq_len_, -1)\n",
    "        return self.wo(comb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "config = LLamaConfig()\n",
    "attention = LLamaAttention(config)\n",
    "embedding = nn.Embedding(config.vocab_size, config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(56, 12)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.head_dim, attention.local_rank"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def precompute_frq_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    frq = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=frq.device)  # type: ignore\n",
    "    frq = torch.outer(t, frq).float()  # type: ignore\n",
    "    frq_cis = torch.polar(torch.ones_like(frq), frq)  # complex64\n",
    "    return frq_cis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "frq = precompute_frq_cis(attention.head_dim, config.max_sentence_length * 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 80])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "embedded = embedding(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 80, 680])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "start_pos = 0\n",
    "mask = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n          [0., 0., -inf,  ..., -inf, -inf, -inf],\n          [0., 0., 0.,  ..., -inf, -inf, -inf],\n          ...,\n          [0., 0., 0.,  ..., 0., -inf, -inf],\n          [0., 0., 0.,  ..., 0., 0., -inf],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full((1, 1, config.max_sentence_length, config.max_sentence_length), float('-inf'), device=x.device)\n",
    "\n",
    "mask = torch.triu(mask, diagonal=start_pos + 1).type_as(embedded)\n",
    "mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:xq : torch.Size([12, 80, 12, 56]) \n",
      "xv : torch.Size([12, 80, 12, 56])\n",
      "xk : torch.Size([12, 80, 12, 56])\n",
      "DEBUG:__main__:x Shape at broadcast_shaping torch.Size([12, 80, 12, 28])\n",
      "DEBUG:__main__:frq Shape at broadcast_shaping torch.Size([1024, 28])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m at \u001B[38;5;241m=\u001B[39m \u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_pos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1488\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1483\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1484\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1489\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1490\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[26], line 27\u001B[0m, in \u001B[0;36mLLamaAttention.forward\u001B[1;34m(self, x, pos_start, frq, mask)\u001B[0m\n\u001B[0;32m     25\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxq : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mxq\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mxv : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mxv\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mxk : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mxk\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# using rotary embedding for key and query\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m xq, xk \u001B[38;5;241m=\u001B[39m \u001B[43mapply_rotary_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# we need to cash key and values\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcash_v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcash_v\u001B[38;5;241m.\u001B[39mto(xv)\n",
      "Cell \u001B[1;32mIn[21], line 37\u001B[0m, in \u001B[0;36mapply_rotary_emb\u001B[1;34m(xq, xk, freqs_cis)\u001B[0m\n\u001B[0;32m     35\u001B[0m xq_ \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_complex(xq\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mxq\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m     36\u001B[0m xk_ \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_complex(xk\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mxk\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m---> 37\u001B[0m freqs_cis \u001B[38;5;241m=\u001B[39m \u001B[43mreshape_for_broadcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxq_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m xq_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(xq_ \u001B[38;5;241m*\u001B[39m freqs_cis)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m     39\u001B[0m xk_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(xk_ \u001B[38;5;241m*\u001B[39m freqs_cis)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m3\u001B[39m)\n",
      "Cell \u001B[1;32mIn[21], line 25\u001B[0m, in \u001B[0;36mreshape_for_broadcast\u001B[1;34m(freqs_cis, x)\u001B[0m\n\u001B[0;32m     23\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrq Shape at broadcast_shaping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfrq\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<\u001B[39m ndim\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m freqs_cis\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     26\u001B[0m shape \u001B[38;5;241m=\u001B[39m [d \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m ndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape)]\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m freqs_cis\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m*\u001B[39mshape)\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "at = attention(embedded, start_pos, frq, mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# if you follow your anger you power will answer ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}