{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikihow/default (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to C:\\Users\\Erfun\\.cache\\huggingface\\datasets\\wikihow\\default\\0.0.0\\cfb412ca2191fac028cae9a5a9a03ba21b08ff2b4bf46f8a0473d7303a3e3683...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BuilderConfig' object has no attribute 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnlp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m----> 2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwikihow\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mE:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mProgramming\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mPython\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mAi-Projects\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mMODEL P\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mipynb\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize of train dataset: \u001B[39m\u001B[38;5;124m\"\u001B[39m, dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32mE:\\Conda3\\lib\\site-packages\\nlp\\load.py:548\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m builder_cls(\n\u001B[0;32m    537\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m    538\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m    545\u001B[0m )\n\u001B[0;32m    547\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[1;32m--> 548\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_verifications\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_verifications\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[0;32m    553\u001B[0m ds \u001B[38;5;241m=\u001B[39m builder_instance\u001B[38;5;241m.\u001B[39mas_dataset(split\u001B[38;5;241m=\u001B[39msplit, ignore_verifications\u001B[38;5;241m=\u001B[39mignore_verifications)\n",
      "File \u001B[1;32mE:\\Conda3\\lib\\site-packages\\nlp\\builder.py:462\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m    460\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not on Hf google storage. Downloading and preparing it from source\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m downloaded_from_gcs:\n\u001B[1;32m--> 462\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[0;32m    463\u001B[0m         dl_manager\u001B[38;5;241m=\u001B[39mdl_manager, verify_infos\u001B[38;5;241m=\u001B[39mverify_infos, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs\n\u001B[0;32m    464\u001B[0m     )\n\u001B[0;32m    465\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32mE:\\Conda3\\lib\\site-packages\\nlp\\builder.py:518\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m    516\u001B[0m split_dict \u001B[38;5;241m=\u001B[39m SplitDict(dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m    517\u001B[0m split_generators_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001B[1;32m--> 518\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split_generators(dl_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msplit_generators_kwargs)\n\u001B[0;32m    519\u001B[0m \u001B[38;5;66;03m# Checksums verification\u001B[39;00m\n\u001B[0;32m    520\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_infos:\n",
      "File \u001B[1;32mE:\\Conda3\\lib\\site-packages\\nlp\\datasets\\wikihow\\cfb412ca2191fac028cae9a5a9a03ba21b08ff2b4bf46f8a0473d7303a3e3683\\wikihow.py:134\u001B[0m, in \u001B[0;36mWikihow._split_generators\u001B[1;34m(self, dl_manager)\u001B[0m\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n\u001B[0;32m    131\u001B[0m             titles[k]\u001B[38;5;241m.\u001B[39madd(line\u001B[38;5;241m.\u001B[39mstrip())\n\u001B[0;32m    133\u001B[0m path_to_manual_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m--> 134\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexpanduser(dl_manager\u001B[38;5;241m.\u001B[39mmanual_dir)), \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename\u001B[49m\n\u001B[0;32m    135\u001B[0m )\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(path_to_manual_file):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m    139\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m does not exist. Make sure you insert a manual dir via `nlp.load_dataset(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwikihow\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, data_dir=...)` that includes a file name \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. Manual download instructions: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    140\u001B[0m             path_to_manual_file, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mfilename, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions\n\u001B[0;32m    141\u001B[0m         )\n\u001B[0;32m    142\u001B[0m     )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'BuilderConfig' object has no attribute 'filename'"
     ]
    }
   ],
   "source": [
    "from nlp import load_dataset\n",
    "dataset = load_dataset('wikihow', data_dir=r'E:\\Programming\\Python\\Ai-Projects\\MODEL P\\ipynb\\data')\n",
    "print(dataset.keys())\n",
    "print(\"Size of train dataset: \", dataset['train'].shape)\n",
    "print(\"Size of Validation dataset: \", dataset['validation'].shape)\n",
    "## Look at Sample Examples\n",
    "print(dataset['train'][0].keys())\n",
    "print(\" Example of text: \", dataset['train'][0]['text'])\n",
    "print(\" Example of Summary: \", dataset['train'][0]['headline'])\n",
    "## Estimate Average Length of Text and Summary\n",
    "tiny_dataset = dataset['train'].select(list(range(0, 100)))\n",
    "text_len = []\n",
    "summary_len=[]\n",
    "for i in range(len(tiny_dataset)):\n",
    "    example = tiny_dataset[i]\n",
    "    text_example = example['text']\n",
    "    text_example = text_example.replace('\\n','')\n",
    "    text_words = text_example.split()\n",
    "    text_len.append(len(text_words))\n",
    "    summary_example = example['headline']\n",
    "    summary_example = summary_example.replace('\\n','')\n",
    "    summary_words = summary_example.split()\n",
    "    summary_len.append(len(summary_words))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(text_len)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}