{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Erfun\\Documents\\news_summary.csv')\n",
    "\n",
    "df[\"text\"] = \"summarize: \" + df[\"text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console(record=True)\n",
    "\n",
    "\n",
    "def display_df(df):\n",
    "    console = Console()\n",
    "    table = Table(Column(\"source_text\", justify=\"center\"), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",\n",
    "                  pad_edge=False, box=box.ASCII)\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\"),\n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"),\n",
    "                        title=\"Training Status\", pad_edge=False, box=box.ASCII)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "\n",
    "class YourDataSetClass(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length=self.source_len, pad_to_max_length=True,\n",
    "                                                  truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length=self.summ_len, pad_to_max_length=True,\n",
    "                                                  truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long),\n",
    "            'source_mask': source_mask.to(dtype=torch.long),\n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype=torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                max_length=150,\n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in\n",
    "                     generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n",
    "            if _ % 10 == 0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"):\n",
    "    torch.manual_seed(model_params[\"SEED\"])\n",
    "    np.random.seed(model_params[\"SEED\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration(T5Config(num_layers=4))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.8\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "                                    model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "                               model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "    #\n",
    "    # for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "    #     train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "    #\n",
    "    # console.log(f\"[Saving Model]...\\n\")\n",
    "    # #Saving the model after training\n",
    "    # path = os.path.join(output_dir, \"model_files\")\n",
    "    # model.save_pretrained(path)\n",
    "    # tokenizer.save_pretrained(path)\n",
    "    #\n",
    "    #\n",
    "    # # evaluating test dataset\n",
    "    # console.log(f\"[Initiating Validation]...\\n\")\n",
    "    # for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    #   predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    #   final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    #   final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "    #\n",
    "    # console.save_text(os.path.join(output_dir,'logs.txt'))\n",
    "    #\n",
    "    # console.log(f\"[Validation Completed.]\\n\")\n",
    "    # console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "    # console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
    "    # console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 50,  # max length of target text\n",
    "    \"SEED\": 42  # set seed for reproducibility\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T5Trainer(dataframe=df[:500], source_text=\"text\", target_text=\"headlines\", model_params=model_params,\n",
    "          output_dir=\"outputs\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/t5-base/resolve/main/spiece.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Conda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "source_text = \"text\"\n",
    "target_text = \"headlines\"\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[3m                                                    Sample Data                                                    \u001B[0m\n+-----------------------------------------------------------------------------------------------------------------+\n|\u001B[1m                      source_text                      \u001B[0m\u001B[1m \u001B[0m|\u001B[1m \u001B[0m\u001B[1m                      target_text                      \u001B[0m|\n|--------------------------------------------------------+--------------------------------------------------------|\n|   summarize: Saurav Kant, an alumnus of upGrad and     |  upGrad learner switches to career in ML & Al with 90% |\n|IIIT-B's PG Program in Machine learning and Artificial  |                       salary hike                      |\n|Intelligence, was a Sr Systems Engineer at Infosys with |                                                        |\n|  almost 5 years of work experience. The program and    |                                                        |\n|     upGrad's 360-degree career support helped him      |                                                        |\n| transition to a Data Scientist at Tech Mahindra with   |                                                        |\n|  90% salary hike. upGrad's Online Power Learning has   |                                                        |\n|               powered 3 lakh+ careers.                 |                                                        |\n|   summarize: Kunal Shah's credit card bill payment     | Delhi techie wins free food from Swiggy for one year on|\n| platform, CRED, gave users a chance to win free food   |                          CRED                          |\n|   from Swiggy for one year. Pranav Kaushik, a Delhi    |                                                        |\n|  techie, bagged this reward after spending 2000 CRED   |                                                        |\n|coins. Users get one CRED coin per rupee of bill paid,  |                                                        |\n|  which can be used to avail rewards from brands like   |                                                        |\n|    Ixigo, BookMyShow, UberEats, Cult.Fit and more.     |                                                        |\n+-----------------------------------------------------------------------------------------------------------------+\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n+-----------------------------------------------------------------------------------------------------------------+\n|<span style=\"font-weight: bold\">                      source_text                       </span>|<span style=\"font-weight: bold\">                       target_text                      </span>|\n|--------------------------------------------------------+--------------------------------------------------------|\n|   summarize: Saurav Kant, an alumnus of upGrad and     |  upGrad learner switches to career in ML &amp; Al with 90% |\n|IIIT-B's PG Program in Machine learning and Artificial  |                       salary hike                      |\n|Intelligence, was a Sr Systems Engineer at Infosys with |                                                        |\n|  almost 5 years of work experience. The program and    |                                                        |\n|     upGrad's 360-degree career support helped him      |                                                        |\n| transition to a Data Scientist at Tech Mahindra with   |                                                        |\n|  90% salary hike. upGrad's Online Power Learning has   |                                                        |\n|               powered 3 lakh+ careers.                 |                                                        |\n|   summarize: Kunal Shah's credit card bill payment     | Delhi techie wins free food from Swiggy for one year on|\n| platform, CRED, gave users a chance to win free food   |                          CRED                          |\n|   from Swiggy for one year. Pranav Kaushik, a Delhi    |                                                        |\n|  techie, bagged this reward after spending 2000 CRED   |                                                        |\n|coins. Users get one CRED coin per rupee of bill paid,  |                                                        |\n|  which can be used to avail rewards from brands like   |                                                        |\n|    Ixigo, BookMyShow, UberEats, Cult.Fit and more.     |                                                        |\n+-----------------------------------------------------------------------------------------------------------------+\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FULL Dataset: \u001B[1m(\u001B[0m\u001B[1;36m98401\u001B[0m, \u001B[1;36m2\u001B[0m\u001B[1m)\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98401</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TRAIN Dataset: \u001B[1m(\u001B[0m\u001B[1;36m78721\u001B[0m, \u001B[1;36m2\u001B[0m\u001B[1m)\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78721</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TEST Dataset: \u001B[1m(\u001B[0m\u001B[1;36m19680\u001B[0m, \u001B[1;36m2\u001B[0m\u001B[1m)\u001B[0m\n\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19680</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe = df[[source_text, target_text]]\n",
    "display_df(dataframe.head(2))\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "train_size = 0.8\n",
    "train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "                                model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "                           model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "training_loader = DataLoader(training_set, **train_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "data = training_set.__getitem__(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "y = data['target_ids'].to(device, dtype=torch.long)\n",
    "y_ids = y[ :-1].contiguous()\n",
    "lm_labels = y[ 1:].clone().detach()\n",
    "lm_labels[y[1:] == tokenizer.pad_token_id] = -100\n",
    "ids = data['source_ids'].to(device, dtype=torch.long)\n",
    "mask = data['source_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "# outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([50])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([49])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ids.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  493,   172,    32,     7,   974,    72,  5987,    16,  1233,   145,\n        11284,    13,     3,  4271,  9352,     1,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n       device='cuda:0')"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}