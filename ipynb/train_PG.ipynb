{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config ,AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Tokens:\n",
    "    eos = '<|endoftext|>'\n",
    "    pad = '<|pad|>'\n",
    "    sos = '<|startoftext|>'\n",
    "\n",
    "\n",
    "class GPT2Dataset(Dataset, Tokens):\n",
    "\n",
    "    def __init__(self, txt_list: typing.Optional[typing.List[str]], tokenizer, max_length: typing.Optional[int] = 768):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(self.sos + txt + self.eos, truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "    def encode(self, text):\n",
    "        enc_trg = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            max_length=self.chunk,\n",
    "            padding='do_not_pad',\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return enc_trg\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mxl = 256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2/resolve/main/vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=Tokens.sos, eos_token=Tokens.eos, pad_token=Tokens.pad)\n",
    "model = GPT2LMHeadModel(\n",
    "    config=GPT2Config(vocab_size=tokenizer.vocab_size + 3, bos_token_id=tokenizer.bos_token_id, max_length=mxl + 1,\n",
    "                      eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, n_positions=mxl + 1,\n",
    "                      n_layer=8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50260, 768)\n",
      "    (wpe): Embedding(257, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-7): 8 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data = open('../data/PGT-DATA-V2.txt', 'r', encoding='utf8').read()\n",
    "data_list = data.split(Tokens.eos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset = GPT2Dataset(data_list, tokenizer, mxl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "batch = 2\n",
    "\n",
    "loader = DataLoader(dataset=dataset, batch_size=batch)\n",
    "epochs = 10\n",
    "# device = 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),4e-4)\n",
    "# print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "686eef9bcb924c5aacd97dfb01cae264"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10140 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9129988e657b4d298178fa2b8d5708c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_6536\\3630344399.py\u001B[0m:\u001B[94m4\u001B[0m in \u001B[92m<module>\u001B[0m                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_6536\\\\3630344399.py'\u001B[0m                          \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mTypeError: \u001B[0munsupported operand \u001B[1;35mtype\u001B[0m\u001B[1m(\u001B[0ms\u001B[1m)\u001B[0m for \u001B[35m/\u001B[0m\u001B[35m/\u001B[0m: \u001B[32m'int'\u001B[0m and \u001B[32m'list'\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_6536\\3630344399.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_6536\\\\3630344399.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>unsupported operand <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">type</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> for <span style=\"color: #800080; text-decoration-color: #800080\">//</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'int'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'list'</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(range(epochs)) as ep_bar:\n",
    "    for epoch in ep_bar:\n",
    "        ep_bar.set_description(f'epoch : {epoch} / {epochs}')\n",
    "        with tqdm(loader,total=dataset.__len__()//batch) as progress_bar:\n",
    "            for batch in progress_bar:\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_labels = batch[0].to(device)\n",
    "                b_masks = batch[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(b_input_ids,\n",
    "                                labels=b_labels,\n",
    "                                attention_mask=b_masks,\n",
    "                                token_type_ids=None\n",
    "                                )\n",
    "                loss = outputs[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_description(f'loss : {loss.item()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}