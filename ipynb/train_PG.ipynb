{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2Tokenizer, GPTJForCausalLM, GPTJConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Tokens:\n",
    "    eos = '<|endoftext|>'\n",
    "    pad = '<|pad|>'\n",
    "    sos = '<|startoftext|>'\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset, Tokens):\n",
    "\n",
    "    def __init__(self, txt_list: typing.Optional[typing.List[str]], tokenizer, max_length: typing.Optional[int] = 768):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(self.sos + txt + self.eos, truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "    def encode(self, text):\n",
    "        enc_trg = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            max_length=self.chunk,\n",
    "            padding='do_not_pad',\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return enc_trg\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mxl = 256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2/resolve/main/vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=Tokens.sos, eos_token=Tokens.eos, pad_token=Tokens.pad)\n",
    "model = GPTJForCausalLM(\n",
    "    config=GPTJConfig(vocab_size=tokenizer.vocab_size + 3, bos_token_id=tokenizer.bos_token_id, max_length=mxl,\n",
    "                      eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, n_positions=mxl,\n",
    "                      n_layer=8, n_head=6, n_embd=600))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(50260, 600)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-7): 8 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=600, out_features=600, bias=False)\n",
      "          (v_proj): Linear(in_features=600, out_features=600, bias=False)\n",
      "          (q_proj): Linear(in_features=600, out_features=600, bias=False)\n",
      "          (out_proj): Linear(in_features=600, out_features=600, bias=False)\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear(in_features=600, out_features=2400, bias=True)\n",
      "          (fc_out): Linear(in_features=2400, out_features=600, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=600, out_features=50260, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = open('../data/PGT-DATA-V2.txt', 'r', encoding='utf8').read()\n",
    "data_list = data.split(Tokens.eos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dataset = GPTDataset(data_list, tokenizer, mxl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "batch = 2\n",
    "\n",
    "loader = DataLoader(dataset=dataset, batch_size=batch)\n",
    "epochs = 10\n",
    "# device = 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 4e-4)\n",
    "# print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.95706\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters())/1e6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtqdm\u001B[49m(\u001B[38;5;28mrange\u001B[39m(epochs)) \u001B[38;5;28;01mas\u001B[39;00m ep_bar:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m ep_bar:\n\u001B[0;32m      3\u001B[0m         ep_bar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "with tqdm(range(epochs)) as ep_bar:\n",
    "    for epoch in ep_bar:\n",
    "        ep_bar.set_description(f'epoch : {epoch} / {epochs}')\n",
    "        with tqdm(loader, total=dataset.__len__() // batch) as progress_bar:\n",
    "            for batch in progress_bar:\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_labels = batch[0].to(device)\n",
    "                b_masks = batch[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(b_input_ids,\n",
    "                                labels=b_labels,\n",
    "                                attention_mask=b_masks,\n",
    "                                token_type_ids=None\n",
    "                                )\n",
    "                loss = outputs[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_description(f'loss : {loss.item()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}