{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a75d2c00589046dbae3098c4a719fa4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Conda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Erfun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.52G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7a4a301111c4750905972adeea14be6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/generation_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b2405bf4ab749abaad2e0e251b30865"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/vocab.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a98b6d985d84f8da784dd81b4d78e29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/merges.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbfd89d56004496ab4a97fb3c119ed14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/added_tokens.json\n",
      "https://huggingface.co/gpt2-medium/resolve/main/special_tokens_map.json\n",
      "https://huggingface.co/gpt2-medium/resolve/main/tokenizer_config.json\n",
      "*init_inputs : ()\n",
      "**init_kwargs : [{'model_max_length': 1024, 'vocab_file': 'C:\\\\Users\\\\Erfun/.cache\\\\huggingface\\\\hub\\\\models--gpt2-medium\\\\snapshots\\\\425b0cc90498ac177aa51ba07be26fc2fea6af9d\\\\vocab.json', 'merges_file': 'C:\\\\Users\\\\Erfun/.cache\\\\huggingface\\\\hub\\\\models--gpt2-medium\\\\snapshots\\\\425b0cc90498ac177aa51ba07be26fc2fea6af9d\\\\merges.txt', 'special_tokens_map_file': None, 'name_or_path': 'gpt2-medium'}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "prompt = model(inp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_18176\\2777959610.py\u001B[0m:\u001B[94m1\u001B[0m in \u001B[92m<module>\u001B[0m                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18176\\\\2777959610.py'\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m:\u001B[94m3469\u001B[0m in \u001B[92mdecode\u001B[0m               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3466 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Convert inputs to python lists\u001B[0m                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3467 \u001B[0m\u001B[2m│   │   \u001B[0mtoken_ids = to_py_obj(token_ids)                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3468 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m3469 \u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mself\u001B[0m._decode(                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3470 \u001B[0m\u001B[2m│   │   │   \u001B[0mtoken_ids=token_ids,                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3471 \u001B[0m\u001B[2m│   │   │   \u001B[0mskip_special_tokens=skip_special_tokens,                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3472 \u001B[0m\u001B[2m│   │   │   \u001B[0mclean_up_tokenization_spaces=clean_up_tokenization_spaces,                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m:\u001B[94m931\u001B[0m in \u001B[92m_decode\u001B[0m                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m928 \u001B[0m\u001B[2m│   \u001B[0m) -> \u001B[96mstr\u001B[0m:                                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m929 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[96mself\u001B[0m._decode_use_source_tokenizer = kwargs.pop(\u001B[33m\"\u001B[0m\u001B[33muse_source_tokenizer\u001B[0m\u001B[33m\"\u001B[0m, \u001B[94mFalse\u001B[0m)      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m930 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m931 \u001B[2m│   │   \u001B[0mfiltered_tokens = \u001B[96mself\u001B[0m.convert_ids_to_tokens(token_ids, skip_special_tokens=skip   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m932 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m933 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# To avoid mixing byte-level and unicode for byte-level BPT\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m934 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# we need to build string separately for added tokens and byte-level tokens\u001B[0m        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m:\u001B[94m906\u001B[0m in \u001B[92mconvert_ids_to_tokens\u001B[0m      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m903 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mself\u001B[0m._convert_id_to_token(ids)                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m904 \u001B[0m\u001B[2m│   │   \u001B[0mtokens = []                                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m905 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mfor\u001B[0m index \u001B[95min\u001B[0m ids:                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m906 \u001B[2m│   │   │   \u001B[0mindex = \u001B[96mint\u001B[0m(index)                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m907 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mif\u001B[0m skip_special_tokens \u001B[95mand\u001B[0m index \u001B[95min\u001B[0m \u001B[96mself\u001B[0m.all_special_ids:                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m908 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mcontinue\u001B[0m                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m909 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mif\u001B[0m index \u001B[95min\u001B[0m \u001B[96mself\u001B[0m.added_tokens_decoder:                                         \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mValueError: \u001B[0minvalid literal for \u001B[1;35mint\u001B[0m\u001B[1m(\u001B[0m\u001B[1m)\u001B[0m with base \u001B[1;36m10\u001B[0m: \u001B[32m'logits'\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_18176\\2777959610.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18176\\\\2777959610.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3469</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3466 │   │   # Convert inputs to python lists</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3467 │   │   </span>token_ids = to_py_obj(token_ids)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3468 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3469 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3470 │   │   │   </span>token_ids=token_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3471 │   │   │   </span>skip_special_tokens=skip_special_tokens,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3472 │   │   │   </span>clean_up_tokenization_spaces=clean_up_tokenization_spaces,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">931</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_decode</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">928 │   </span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">929 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode_use_source_tokenizer = kwargs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"use_source_tokenizer\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">930 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>931 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>filtered_tokens = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_ids_to_tokens(token_ids, skip_special_tokens=skip   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">932 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">933 │   │   # To avoid mixing byte-level and unicode for byte-level BPT</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">934 │   │   # we need to build string separately for added tokens and byte-level tokens</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">906</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_ids_to_tokens</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">903 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._convert_id_to_token(ids)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">904 │   │   </span>tokens = []                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">905 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> ids:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>906 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>index = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(index)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">907 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> skip_special_tokens <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.all_special_ids:                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">908 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">continue</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">909 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.added_tokens_decoder:                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>invalid literal for <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">int</span><span style=\"font-weight: bold\">()</span> with base <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'logits'</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.decode(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['logits', 'past_key_values']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, v in prompt.items()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_18176\\533735341.py\u001B[0m:\u001B[94m5\u001B[0m in \u001B[92m<module>\u001B[0m                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18176\\\\533735341.py'\u001B[0m                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m:\u001B[94m3469\u001B[0m in \u001B[92mdecode\u001B[0m               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3466 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Convert inputs to python lists\u001B[0m                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3467 \u001B[0m\u001B[2m│   │   \u001B[0mtoken_ids = to_py_obj(token_ids)                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3468 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m3469 \u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mself\u001B[0m._decode(                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3470 \u001B[0m\u001B[2m│   │   │   \u001B[0mtoken_ids=token_ids,                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3471 \u001B[0m\u001B[2m│   │   │   \u001B[0mskip_special_tokens=skip_special_tokens,                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m3472 \u001B[0m\u001B[2m│   │   │   \u001B[0mclean_up_tokenization_spaces=clean_up_tokenization_spaces,                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m:\u001B[94m931\u001B[0m in \u001B[92m_decode\u001B[0m                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m928 \u001B[0m\u001B[2m│   \u001B[0m) -> \u001B[96mstr\u001B[0m:                                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m929 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[96mself\u001B[0m._decode_use_source_tokenizer = kwargs.pop(\u001B[33m\"\u001B[0m\u001B[33muse_source_tokenizer\u001B[0m\u001B[33m\"\u001B[0m, \u001B[94mFalse\u001B[0m)      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m930 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m931 \u001B[2m│   │   \u001B[0mfiltered_tokens = \u001B[96mself\u001B[0m.convert_ids_to_tokens(token_ids, skip_special_tokens=skip   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m932 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m933 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# To avoid mixing byte-level and unicode for byte-level BPT\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m934 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# we need to build string separately for added tokens and byte-level tokens\u001B[0m        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m:\u001B[94m906\u001B[0m in \u001B[92mconvert_ids_to_tokens\u001B[0m      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m903 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mself\u001B[0m._convert_id_to_token(ids)                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m904 \u001B[0m\u001B[2m│   │   \u001B[0mtokens = []                                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m905 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mfor\u001B[0m index \u001B[95min\u001B[0m ids:                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m906 \u001B[2m│   │   │   \u001B[0mindex = \u001B[96mint\u001B[0m(index)                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m907 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mif\u001B[0m skip_special_tokens \u001B[95mand\u001B[0m index \u001B[95min\u001B[0m \u001B[96mself\u001B[0m.all_special_ids:                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m908 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mcontinue\u001B[0m                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m909 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mif\u001B[0m index \u001B[95min\u001B[0m \u001B[96mself\u001B[0m.added_tokens_decoder:                                         \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mTypeError: \u001B[0m\u001B[1;35mint\u001B[0m\u001B[1m(\u001B[0m\u001B[1m)\u001B[0m argument must be a string, a bytes-like object or a number, not \u001B[32m'list'\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_18176\\533735341.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18176\\\\533735341.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3469</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3466 │   │   # Convert inputs to python lists</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3467 │   │   </span>token_ids = to_py_obj(token_ids)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3468 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3469 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3470 │   │   │   </span>token_ids=token_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3471 │   │   │   </span>skip_special_tokens=skip_special_tokens,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3472 │   │   │   </span>clean_up_tokenization_spaces=clean_up_tokenization_spaces,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">931</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_decode</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">928 │   </span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">929 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode_use_source_tokenizer = kwargs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"use_source_tokenizer\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">930 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>931 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>filtered_tokens = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_ids_to_tokens(token_ids, skip_special_tokens=skip   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">932 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">933 │   │   # To avoid mixing byte-level and unicode for byte-level BPT</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">934 │   │   # we need to build string separately for added tokens and byte-level tokens</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">906</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_ids_to_tokens</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">903 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._convert_id_to_token(ids)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">904 │   │   </span>tokens = []                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">905 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> ids:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>906 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>index = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(index)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">907 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> skip_special_tokens <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.all_special_ids:                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">908 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">continue</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">909 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.added_tokens_decoder:                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">int</span><span style=\"font-weight: bold\">()</span> argument must be a string, a bytes-like object or a number, not <span style=\"color: #008000; text-decoration-color: #008000\">'list'</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = tokenizer.encode('hello how are you ', return_tensors='pt', ).cuda()\n",
    "generated = inp\n",
    "past = None\n",
    "for i in range(20):\n",
    "    print(\"{}=>>{}\".format(i, tokenizer.decode(generated)))\n",
    "    output, past = model(inp, past=past)\n",
    "    token = torch.argmax(output[0, :])\n",
    "    generated += [token.tolist()]\n",
    "    context = token.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/gpt2-medium/resolve/main/vocab.json\n",
      "*init_inputs : ()\n",
      "**init_kwargs : [{'model_max_length': 1024, 'vocab_file': 'C:\\\\Users\\\\Erfun/.cache\\\\huggingface\\\\hub\\\\models--gpt2-medium\\\\snapshots\\\\425b0cc90498ac177aa51ba07be26fc2fea6af9d\\\\vocab.json', 'merges_file': 'C:\\\\Users\\\\Erfun/.cache\\\\huggingface\\\\hub\\\\models--gpt2-medium\\\\snapshots\\\\425b0cc90498ac177aa51ba07be26fc2fea6af9d\\\\merges.txt', 'special_tokens_map_file': None, 'name_or_path': 'gpt2-medium'}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Encode a text inputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you? what are you doing here?\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "hello how are you? what are you doing here?\n",
      "\n",
      "I'm here to tell you that you're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n",
      "\n",
      "You're not alone.\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_18176\\2862210961.py\u001B[0m:\u001B[94m12\u001B[0m in \u001B[92m<module>\u001B[0m                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18176\\\\2862210961.py'\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m:\u001B[94m1482\u001B[0m in \u001B[92m_call_impl\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1479 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._backward_pre_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1480 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_backward_pre_hooks \u001B[95mor\u001B[0m _global_backward_hooks                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1481 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1482 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*args, **kwargs)                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1483 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1484 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1485 \u001B[0m\u001B[2m│   │   \u001B[0mbackward_pre_hooks = []                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001B[0m:\u001B[94m1043\u001B[0m in \u001B[92mforward\u001B[0m            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1040 \u001B[0m\u001B[2;33m│   │   \u001B[0m\u001B[33m\"\"\"\u001B[0m                                                                               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1041 \u001B[0m\u001B[2m│   │   \u001B[0mreturn_dict = return_dict \u001B[94mif\u001B[0m return_dict \u001B[95mis\u001B[0m \u001B[95mnot\u001B[0m \u001B[94mNone\u001B[0m \u001B[94melse\u001B[0m \u001B[96mself\u001B[0m.config.use_return  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1042 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1043 \u001B[2m│   │   \u001B[0mtransformer_outputs = \u001B[96mself\u001B[0m.transformer(                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1044 \u001B[0m\u001B[2m│   │   │   \u001B[0minput_ids,                                                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1045 \u001B[0m\u001B[2m│   │   │   \u001B[0mpast_key_values=past_key_values,                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1046 \u001B[0m\u001B[2m│   │   │   \u001B[0mattention_mask=attention_mask,                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m:\u001B[94m1482\u001B[0m in \u001B[92m_call_impl\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1479 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._backward_pre_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1480 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_backward_pre_hooks \u001B[95mor\u001B[0m _global_backward_hooks                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1481 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1482 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*args, **kwargs)                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1483 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1484 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1485 \u001B[0m\u001B[2m│   │   \u001B[0mbackward_pre_hooks = []                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001B[0m:\u001B[94m887\u001B[0m in \u001B[92mforward\u001B[0m             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 884 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mencoder_attention_mask,                                               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 885 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m)                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 886 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 887 \u001B[2m│   │   │   │   \u001B[0moutputs = block(                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 888 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mhidden_states,                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 889 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mlayer_past=layer_past,                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 890 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mattention_mask=attention_mask,                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m:\u001B[94m1482\u001B[0m in \u001B[92m_call_impl\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1479 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._backward_pre_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1480 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_backward_pre_hooks \u001B[95mor\u001B[0m _global_backward_hooks                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1481 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1482 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*args, **kwargs)                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1483 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1484 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1485 \u001B[0m\u001B[2m│   │   \u001B[0mbackward_pre_hooks = []                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001B[0m:\u001B[94m388\u001B[0m in \u001B[92mforward\u001B[0m             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 385 \u001B[0m\u001B[2m│   \u001B[0m) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 386 \u001B[0m\u001B[2m│   │   \u001B[0mresidual = hidden_states                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 387 \u001B[0m\u001B[2m│   │   \u001B[0mhidden_states = \u001B[96mself\u001B[0m.ln_1(hidden_states)                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 388 \u001B[2m│   │   \u001B[0mattn_outputs = \u001B[96mself\u001B[0m.attn(                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 389 \u001B[0m\u001B[2m│   │   │   \u001B[0mhidden_states,                                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 390 \u001B[0m\u001B[2m│   │   │   \u001B[0mlayer_past=layer_past,                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 391 \u001B[0m\u001B[2m│   │   │   \u001B[0mattention_mask=attention_mask,                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m:\u001B[94m1482\u001B[0m in \u001B[92m_call_impl\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1479 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._backward_pre_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1480 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_backward_pre_hooks \u001B[95mor\u001B[0m _global_backward_hooks                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1481 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1482 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*args, **kwargs)                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1483 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1484 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1485 \u001B[0m\u001B[2m│   │   \u001B[0mbackward_pre_hooks = []                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001B[0m:\u001B[94m329\u001B[0m in \u001B[92mforward\u001B[0m             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 326 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m.reorder_and_upcast_attn:                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 327 \u001B[0m\u001B[2m│   │   │   \u001B[0mattn_output, attn_weights = \u001B[96mself\u001B[0m._upcast_and_reordered_attn(query, key, valu  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 328 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 329 \u001B[2m│   │   │   \u001B[0mattn_output, attn_weights = \u001B[96mself\u001B[0m._attn(query, key, value, attention_mask, he  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 330 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 331 \u001B[0m\u001B[2m│   │   \u001B[0mattn_output = \u001B[96mself\u001B[0m._merge_heads(attn_output, \u001B[96mself\u001B[0m.num_heads, \u001B[96mself\u001B[0m.head_dim)       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 332 \u001B[0m\u001B[2m│   │   \u001B[0mattn_output = \u001B[96mself\u001B[0m.c_proj(attn_output)                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mE:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001B[0m:\u001B[94m199\u001B[0m in \u001B[92m_attn\u001B[0m               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 196 \u001B[0m\u001B[2m│   │   │   \u001B[0mmask_value = torch.finfo(attn_weights.dtype).min                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 197 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scala\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 198 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 199 \u001B[2m│   │   │   \u001B[0mmask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_we  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 200 \u001B[0m\u001B[2m│   │   │   \u001B[0mattn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype),  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 201 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 202 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m attention_mask \u001B[95mis\u001B[0m \u001B[95mnot\u001B[0m \u001B[94mNone\u001B[0m:                                                    \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mKeyboardInterrupt\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Erfun\\AppData\\Local\\Temp\\ipykernel_18176\\2862210961.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\Erfun\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18176\\\\2862210961.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1482</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1479 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1480 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1481 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1482 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1483 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1484 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1485 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1043</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1040 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1041 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1043 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1044 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1045 │   │   │   </span>past_key_values=past_key_values,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1046 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1482</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1479 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1480 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1481 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1482 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1483 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1484 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1485 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">887</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 884 │   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 885 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 886 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 887 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = block(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 888 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 889 │   │   │   │   │   </span>layer_past=layer_past,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 890 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1482</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1479 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1480 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1481 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1482 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1483 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1484 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1485 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">388</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 385 │   </span>) -&gt; Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 386 │   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 387 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_1(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 388 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 389 │   │   │   </span>hidden_states,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 390 │   │   │   </span>layer_past=layer_past,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 391 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1482</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1479 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1480 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1481 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1482 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1483 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1484 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1485 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">329</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 326 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.reorder_and_upcast_attn:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 327 │   │   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._upcast_and_reordered_attn(query, key, valu  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 328 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 329 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._attn(query, key, value, attention_mask, he  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 330 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 331 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._merge_heads(attn_output, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 332 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.c_proj(attn_output)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">E:\\Conda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">199</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_attn</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 196 │   │   │   </span>mask_value = torch.finfo(attn_weights.dtype).min                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 197 │   │   │   # Need to be a tensor, otherwise we get error: `RuntimeError: expected scala</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 198 │   │   │   # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 199 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_we  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 200 │   │   │   </span>attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype),  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 201 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 202 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "text = \"hello how are you ? what are you doing\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "for i in range(20):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    predicted_index = torch.argmax(predictions[0, -1, :])\n",
    "\n",
    "    tokens_tensor = torch.cat([tokens_tensor, predicted_index.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    predicted_text = tokenizer.decode(tokens_tensor.squeeze(0).cpu(),skip_special_tokens=True)\n",
    "    print(f'\\r{predicted_text}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "\"hello how are you? what are you doing here?\\n\\nI'm here to tell you that you're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\\nYou're not alone.\\n\""
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}