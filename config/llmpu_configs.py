LLmPU_M = {"d_ff": 768,
           "d_kv": 32,
           "d_model": 512,
           "decoder_start_token_id": 0,
           "dropout_rate": 0.1,
           "eos_token_id": 1,
           "feed_forward_proj": "gated-gelu",
           "initializer_factor": 1.0,
           "is_encoder_decoder": True,
           "layer_norm_epsilon": 1e-06,
           "model_type": "t5",
           "n_positions": 1024,
           "num_decoder_layers": 12,
           "num_heads": 12,
           "num_layers": 8,
           "output_past": True,
           "pad_token_id": 0,
           "relative_attention_max_distance": 128,
           "relative_attention_num_buckets": 32,
           "tie_word_embeddings": False,
           "use_cache": True,
           "max_length": 512,
           "mesh": 5
           }
