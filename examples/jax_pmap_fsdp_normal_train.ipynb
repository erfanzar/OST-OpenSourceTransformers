{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey, split\n",
    "import jax\n",
    "import flax\n",
    "import optax\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM, AutoConfig\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "from jax.experimental.pjit import pjit, with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "from flax.training import train_state\n",
    "from jax import numpy as jnp\n",
    "from jax.sharding import PartitionSpec, Mesh\n",
    "\n",
    "PS = PartitionSpec\n",
    "from jax.interpreters import pxla\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FSDP = True\n",
    "PMAP = False\n",
    "NORM = False\n",
    "max_length = 2048\n",
    "num_epochs = 2\n",
    "batch_size = 4\n",
    "max_steps = None\n",
    "dataset_name = 'erfanzar/Data-LGeM-2048'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, use_auth_token=True)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    rs = {}\n",
    "    for key in batch[0].keys():\n",
    "        ssp = [jnp.array(f[key])[..., -max_length:] for f in batch]\n",
    "        rs[key] = jnp.stack(ssp).reshape(-1, ssp[0].shape[-1])\n",
    "    return rs\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset['train'], collate_fn=collate_fn, batch_size=batch_size, drop_last=True)\n",
    "max_steps = num_epochs * len(dataloader) if max_steps is None else max_steps\n",
    "tokenizer = AutoTokenizer.from_pretrained('erfanzar/FlaxLGeM', trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def match_partition_rules(rules, params):\n",
    "    def get_partition_spec(name, leaf):\n",
    "        if len(leaf.shape) == 0 or np.prod(leaf.shape) == 1:\n",
    "            return PS()\n",
    "        for rule, ps in rules:\n",
    "            if re.search(rule, name) is not None:\n",
    "                return ps\n",
    "        raise ValueError(f'Partition rule not found for param: {name}')\n",
    "\n",
    "    def tree_path_to_string(path):\n",
    "        keys = []\n",
    "        for i, key in enumerate(path):\n",
    "            if isinstance(key, jax.tree_util.SequenceKey):\n",
    "                keys.append(str(key.idx))\n",
    "            elif isinstance(key, (jax.tree_util.DictKey, jax.tree_util.FlattenedIndexKey)):\n",
    "                keys.append(str(key.key))\n",
    "            elif isinstance(key, jax.tree_util.GetAttrKey):\n",
    "                keys.append(str(key.name))\n",
    "            else:\n",
    "                keys.append(str(key))\n",
    "        return '/'.join(keys)\n",
    "\n",
    "    return jax.tree_util.tree_map_with_path(\n",
    "        lambda path, p: get_partition_spec(tree_path_to_string(path), p),\n",
    "        params\n",
    "    )\n",
    "\n",
    "\n",
    "def count_params(_p):\n",
    "    print('\\033[1;31mModel Contain : ',\n",
    "          sum(i.size for i in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9, ' Billion Parameters')\n",
    "\n",
    "\n",
    "def names_in_mesh(*names):\n",
    "    return set(names) <= set(pxla.thread_resources.env.physical_mesh.axis_names)\n",
    "\n",
    "\n",
    "def get_names(partition_specs):\n",
    "    names = set()\n",
    "    for item in partition_specs:\n",
    "        if item is None:\n",
    "            continue\n",
    "        elif isinstance(item, str):\n",
    "            names.add(item)\n",
    "    return list(names)\n",
    "\n",
    "\n",
    "def with_sharding_constraint__a(x, partition_spec):\n",
    "    names = get_names(partition_spec)\n",
    "    if names_in_mesh(*names):\n",
    "        x = with_sharding_constraint(x, partition_spec)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Incase for training LGem model from config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    'erfanzar/FlaxLGeM',\n",
    "    hidden_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_hidden_layers=16,\n",
    "    intermediate_size=8192,\n",
    "    trust_remote_code=True,\n",
    "    vocab_size=32005,\n",
    "    fsdp=True\n",
    ")\n",
    "model = FlaxAutoModelForCausalLM.from_config(config=config, _do_init=False, trust_remote_code=True)\n",
    "\n",
    "scheduler = optax.cosine_decay_schedule(\n",
    "    init_value=1.85e-4,\n",
    "    decay_steps=800,\n",
    ")\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.scale_by_adam(),\n",
    "    optax.add_decayed_weights(1e-1),\n",
    "    optax.scale_by_schedule(scheduler),\n",
    "    optax.scale(-1.0)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_fn():\n",
    "    input_ids = jnp.ones((1, 2048), dtype=jnp.int32)\n",
    "    attention_mask = jnp.ones((1, 2048), dtype=jnp.int32)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    varient = model.module.init(key, input_ids, attention_mask, return_dict=False)\n",
    "    varient = model.to_bf16(varient)\n",
    "    return train_state.TrainState.create(\n",
    "        tx=tx,\n",
    "        params=varient,\n",
    "        apply_fn=model.__call__\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if PMAP:\n",
    "    input_ids = jnp.ones((batch_size, max_length), dtype=jnp.int32)\n",
    "    attention_mask = jnp.ones((batch_size, max_length), dtype=jnp.int32)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    varient = model.module.init(key, input_ids, attention_mask, return_dict=False)\n",
    "    varient = model.to_bf16(varient)\n",
    "    state = train_state.TrainState.create(\n",
    "        tx=tx,\n",
    "        params=varient,\n",
    "        apply_fn=model.__call__\n",
    "    )\n",
    "    state = flax.jax_utils.replicate(state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_step(state, input_ids, attention_mask):\n",
    "    def calculate_loss(params):\n",
    "        logits = state.apply_fn(params=params, attention_mask=attention_mask, input_ids=input_ids,\n",
    "                                return_dict=True).logits\n",
    "        loss_ = optax.softmax_cross_entropy_with_integer_labels(logits=logits[..., 1:, :], labels=input_ids[..., :-1])\n",
    "        return jnp.mean(loss_)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(calculate_loss, has_aux=False)\n",
    "    loss__, grad = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss__\n",
    "\n",
    "\n",
    "def fsdp_train_step(state, input_ids, attention_mask):\n",
    "    input_ids = with_sharding_constraint(input_ids, PS(('dp', 'fsdp')))\n",
    "    attention_mask = with_sharding_constraint(attention_mask, PS(('dp', 'fsdp')))\n",
    "\n",
    "    def calculate_loss(params):\n",
    "        logits = state.apply_fn(params=params, input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                return_dict=True).logits\n",
    "        loss_ = optax.softmax_cross_entropy_with_integer_labels(logits=logits[..., 1:, :], labels=input_ids[..., :-1])\n",
    "        return jnp.mean(loss_)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(calculate_loss, has_aux=False)\n",
    "    loss__, grad = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss__\n",
    "\n",
    "\n",
    "@functools.partial(jax.pmap, axis_name='batch', donate_argnums=(0,))\n",
    "def pmap_train_step(state, input_ids, attention_mask):\n",
    "    def calculate_loss(params):\n",
    "        logits = state.apply_fn(params=params, attention_mask=attention_mask, input_ids=input_ids,\n",
    "                                return_dict=True).logits\n",
    "        loss_ = optax.softmax_cross_entropy_with_integer_labels(logits=logits[..., 1:, :], labels=input_ids[..., :-1])\n",
    "        return jnp.mean(loss_)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(jax.jit(calculate_loss), has_aux=False)\n",
    "    loss__, grad = grad_fn(state.params)\n",
    "    loss__ = jax.lax.pmean(loss__, 'batch')\n",
    "    grad = jax.lax.pmean(grad, 'batch')\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss__\n",
    "\n",
    "\n",
    "def step_prediction(state, input_ids, attention_mask):\n",
    "    logits = state.apply_fn(params=state.params, attention_mask=attention_mask, input_ids=input_ids, return_dict=True)\n",
    "    return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if FSDP:\n",
    "    eval_tree_init_fn = jax.eval_shape(init_fn)\n",
    "    partition_tree = match_partition_rules(config.get_partition_rules(), eval_tree_init_fn)\n",
    "    sharded_init_fn = pjit(\n",
    "        init_fn,\n",
    "        out_shardings=partition_tree\n",
    "    )\n",
    "    sharded_train_step_fn = pjit(\n",
    "        fsdp_train_step,\n",
    "        in_shardings=(partition_tree, PS(), PS()),\n",
    "        out_shardings=(partition_tree, PS())\n",
    "    )\n",
    "    phsycal_mesh = mesh_utils.create_device_mesh((1, 8, 1))\n",
    "    mesh = Mesh(phsycal_mesh, ('dp', 'fsdp', 'mp'))\n",
    "\n",
    "    with mesh:\n",
    "        sharded_state = sharded_init_fn()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if PMAP:\n",
    "    print('PMAP TRAINER .....')\n",
    "    count_params(state.params)\n",
    "if NORM:\n",
    "    print('NORM TRAINER .....')\n",
    "    count_params(state.params)\n",
    "if FSDP:\n",
    "    print('FSDP TRAINER ..... ')\n",
    "    count_params(sharded_state.params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## FSDP TRAIN\n",
    "if FSDP:\n",
    "    with mesh:\n",
    "        pbar = tqdm(total=max_steps)\n",
    "        save_steps = 200\n",
    "        i = 0\n",
    "        losses = []\n",
    "        logging_step = 1\n",
    "        for _ in range(num_epochs):\n",
    "            for batch in dataloader:\n",
    "                i += 1\n",
    "                if i > max_steps:\n",
    "                    break\n",
    "                input_ids = batch['input_ids']\n",
    "                attention_mask = batch['attention_mask']\n",
    "                sharded_state, loss = sharded_train_step_fn(sharded_state, input_ids, attention_mask)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## PMAP TRAIN FUNCTION\n",
    "if PMAP:\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    save_steps = 200\n",
    "    i = 0\n",
    "    losses = []\n",
    "    logging_step = 1\n",
    "    for _ in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            i += 1\n",
    "            if i > max_steps:\n",
    "                break\n",
    "            input_ids = batch['input_ids'].reshape(len(jax.devices()), -1, max_length)\n",
    "            attention_mask = batch['attention_mask'].reshape(len(jax.devices()), -1, max_length)\n",
    "            state, loss = pmap_train_step(state, input_ids, attention_mask)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=loss[0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## NORMAL AND SINGLE THREAD TRAIN\n",
    "if NORM:\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    save_steps = 200\n",
    "    i = 0\n",
    "    losses = []\n",
    "    logging_step = 1\n",
    "    for _ in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            i += 1\n",
    "            if i > max_steps:\n",
    "                break\n",
    "            input_ids = batch['input_ids'].reshape(-1, max_length)\n",
    "            attention_mask = batch['attention_mask'].reshape(-1, max_length)\n",
    "            state, loss = train_step(state, input_ids, attention_mask)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=loss)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
