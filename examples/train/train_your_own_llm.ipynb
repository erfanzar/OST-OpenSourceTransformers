{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey, split\n",
    "import jax\n",
    "import flax\n",
    "import optax\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM, AutoConfig\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "from jax.experimental.pjit import pjit, with_sharding_constraint, PartitionSpec as PS\n",
    "from jax.experimental import mesh_utils\n",
    "from flax.training import train_state\n",
    "from jax import numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from jax.interpreters import pxla\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from flax.serialization import from_bytes, to_bytes, to_state_dict, from_state_dict\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict, empty_node\n",
    "import msgpack\n",
    "import torch\n",
    "from fjutils import match_partition_rules, make_shard_and_gather_fns, float_tensor_to_dtype, StreamingCheckpointer, \\\n",
    "    count_params\n",
    "from huggingface_hub import HfApi\n",
    "from fjutils.utils import get_dataloader\n",
    "\n",
    "api = HfApi()\n",
    "ckpt_stream = StreamingCheckpointer(StreamingCheckpointer.get_default_config(), 'ckpt_dir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 1900\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "max_steps = None\n",
    "sch_linear = True\n",
    "learning_rate = 8e-6\n",
    "learning_rate_end = 4e-6\n",
    "use_adamw_instead_of_lion = True\n",
    "weight_decay = 0.01\n",
    "model_id = \"<MODEL_YOU_WANT_TO_TRAIN_ID>\"  # check available models to use like (FlaxFalcon,FlaxMpt,FlaxLLama,FlaxOpenLLama)\n",
    "ckpt_name = '<YOUR_CKPT_PATH_OR_NAME_(EASYDEL OR OST FORMAT!)>'\n",
    "dataset_name = '<YOUR_DATASET>'\n",
    "repo_id = '<REPO ID TO PUSH MODEL>'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sharding_shape = (1, 8, 1)  # DP , FSDP , MP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataloader, max_steps = get_dataloader(\n",
    "    dataset_or_huggingface_dataset_hub_id=dataset_name,\n",
    "    max_steps=max_steps,\n",
    "    max_length=max_length,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "assert hasattr(config, 'get_partition_rules')\n",
    "model = FlaxAutoModelForCausalLM.from_config(config, trust_remote_code=True, dtype=jnp.bfloat16,\n",
    "                                             param_dtype=jnp.bfloat16,\n",
    "                                             _do_init=False)  # Wr are using bfloat16 since TPUS support bfloat16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if use_adamw_instead_of_lion and sch_linear:\n",
    "    from fjutils.optimizers import get_adamw_with_linear_scheduler\n",
    "\n",
    "    tx, scheduler = get_adamw_with_linear_scheduler(\n",
    "        steps=max_steps,\n",
    "        learning_rate_end=learning_rate_end\n",
    "    )\n",
    "elif use_adamw_instead_of_lion and not sch_linear:\n",
    "    from fjutils.optimizers import get_adamw_with_cosine_scheduler\n",
    "\n",
    "    tx, scheduler = get_adamw_with_cosine_scheduler(\n",
    "        steps=max_steps,\n",
    "        learning_rate=learning_rate_end,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "elif not use_adamw_instead_of_lion and sch_linear:\n",
    "    from fjutils.optimizers import get_lion_with_linear_scheduler\n",
    "\n",
    "    tx, scheduler = get_lion_with_linear_scheduler(\n",
    "        steps=max_steps,\n",
    "        learning_rate_end=learning_rate_end,\n",
    "        learning_rate_start=learning_rate\n",
    "    )\n",
    "elif not use_adamw_instead_of_lion and not sch_linear:\n",
    "    from fjutils.optimizers import get_lion_with_cosine_scheduler\n",
    "\n",
    "    tx, scheduler = get_lion_with_cosine_scheduler(\n",
    "        steps=max_steps,\n",
    "        learning_rate=learning_rate_end,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_fn():\n",
    "    from flax.training import train_state\n",
    "    params = model.init_weights(jax.random.PRNGKey(0), (1, max_length))\n",
    "    params = model.to_bf16(params)\n",
    "    return train_state.TrainState.create(\n",
    "        tx=tx,\n",
    "        params=flax.core.freeze({'params': params}),\n",
    "        apply_fn=model.__call__\n",
    "    )\n",
    "\n",
    "\n",
    "def init_fn_wop():\n",
    "    from flax.training import train_state\n",
    "    params = model.to_fp32(params)  # this is not an error do not change this !\n",
    "    return train_state.TrainState.create(\n",
    "        tx=tx,\n",
    "        params=params,\n",
    "        apply_fn=model.__call__\n",
    "    )\n",
    "\n",
    "\n",
    "def create_train_state_from_params(params_):\n",
    "    from flax.training import train_state\n",
    "    return train_state.TrainState.create(\n",
    "        tx=tx,\n",
    "        apply_fn=model.__call__,\n",
    "        params=params_\n",
    "    )\n",
    "\n",
    "\n",
    "def dummy_init():\n",
    "    from flax.training import train_state\n",
    "    return train_state.TrainState.create(\n",
    "        tx=tx,\n",
    "        apply_fn=model.__call__,\n",
    "        params=None\n",
    "    )\n",
    "\n",
    "\n",
    "def fsdp_train_step(state, batch):\n",
    "    batch = with_sharding_constraint(batch, PS(('dp', 'fsdp')))\n",
    "\n",
    "    def calculate_loss(params):\n",
    "        logits = state.apply_fn(params=params, **batch,\n",
    "                                return_dict=True).logits\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits[..., :-1, :],\n",
    "                                                               labels=batch['input_ids'][..., 1:])\n",
    "        loss = jnp.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(calculate_loss, has_aux=False)\n",
    "    loss__, grad = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss__\n",
    "\n",
    "\n",
    "@functools.partial(jax.pmap, axis_name='batch', donate_argnums=(0,))\n",
    "def pmap_train_step(state, input_ids, attention_mask):\n",
    "    def calculate_loss(params):\n",
    "        logits = state.apply_fn(params=params, attention_mask=attention_mask, input_ids=input_ids,\n",
    "                                return_dict=True).logits\n",
    "        loss_ = optax.softmax_cross_entropy_with_integer_labels(logits=logits[..., 1:, :], labels=input_ids[..., :-1])\n",
    "        return jnp.mean(loss_)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(jax.jit(calculate_loss), has_aux=False)\n",
    "    loss__, grad = grad_fn(state.params)\n",
    "    loss__ = jax.lax.pmean(loss__, 'batch')\n",
    "    grad = jax.lax.pmean(grad, 'batch')\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_state_shape = jax.eval_shape(init_fn)\n",
    "train_state_partition_spec = match_partition_rules(config.get_partition_rules(True), train_state_shape)\n",
    "sharded_init_fn = pjit(init_fn, out_shardings=train_state_partition_spec)\n",
    "sharded_init_f_wop = pjit(init_fn_wop, out_shardings=train_state_partition_spec)\n",
    "sharded_create_from_params_fn = pjit(\n",
    "    create_train_state_from_params,\n",
    "    in_shardings=(train_state_partition_spec.params,),\n",
    "    out_shardings=train_state_partition_spec,\n",
    "    donate_argnums=(0,)\n",
    ")\n",
    "sharded_train_step_fn = pjit(\n",
    "    fsdp_train_step, in_shardings=(train_state_partition_spec, PS()),\n",
    "    out_shardings=(train_state_partition_spec, PS()), donate_argnums=(0, 0, 0), )\n",
    "phsycal_mesh = mesh_utils.create_device_mesh((sharding_shape))\n",
    "mesh = Mesh(phsycal_mesh, ('dp', 'fsdp', 'mp'))\n",
    "with mesh:\n",
    "    shard_fns, ghater_fns = make_shard_and_gather_fns(train_state_partition_spec, jnp.bfloat16)\n",
    "    _, params = ckpt_stream.load_trainstate_checkpoint(\n",
    "        f'params::{ckpt_name}', train_state_shape, shard_fns\n",
    "    )\n",
    "    sharded_train_state_ = sharded_create_from_params_fn(params)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_params(sharded_train_state_.params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    i = 0\n",
    "    losses = []\n",
    "    logging_step = 1\n",
    "    learning_rates = []\n",
    "    for _ in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            i += 1\n",
    "            if i > max_steps:\n",
    "                break\n",
    "            sharded_train_state_, loss = sharded_train_step_fn(sharded_train_state_, batch)\n",
    "            losses.append(loss)\n",
    "            learning_rates.append(scheduler(i).tolist())\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=loss, learning_rate=scheduler(i).tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optional Prediction\n",
    "    heres a simple function to test your model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(state, input_ids):\n",
    "    input_ids = with_sharding_constraint(input_ids, PS(('dp', 'fsdp')))\n",
    "    pred = state.apply_fn(params=state.params, input_ids=input_ids, return_dict=True)\n",
    "    token = jnp.argmax(jax.nn.softmax(pred.logits)[:, -1, :])\n",
    "    input_ids = jnp.concatenate([input_ids, token.reshape(1, -1)], axis=-1)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "sharded_predict = pjit(predict, out_shardings=PS(), in_shardings=(train_state_partition_spec, PS()))\n",
    "text = None  # write down your text :)\n",
    "with mesh:\n",
    "    input_ids = jnp.asarray(tokenizer.encode(text,\n",
    "                                             add_special_tokens=False), dtype='i4').reshape(1, -1)\n",
    "    for i in range(50):\n",
    "        input_ids = sharded_predict(sharded_train_state_, input_ids)\n",
    "        clear_output(wait=True)\n",
    "        print(tokenizer.decode(input_ids[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = f'model_{model_id.split(\"/\")[1]}_ostformat'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    !mkdir ckpt_dir\n",
    "    ckpt_stream.save_checkpoint(sharded_train_state_.params['params'], filename=filename,\n",
    "                                gather_fns=ghater_fns.params['params'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=f'ckpt_dir/{filename}',\n",
    "    repo_id=repo_id,\n",
    "    path_in_repo='filename'\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
