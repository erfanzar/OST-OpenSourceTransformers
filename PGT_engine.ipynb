{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch.utils.data\n",
    "from erutils.command_line_interface import fprint\n",
    "\n",
    "from modules.models import PGT\n",
    "from utils.utils import DatasetPGT, make2d, save_model, get_config_by_name\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "batch = 8\n",
    "percentage = 0.3\n",
    "\n",
    "prp = torch.cuda.get_device_properties(\"cuda\")\n",
    "fprint(\n",
    "    f'DEVICES : {torch.cuda.get_device_name()} | {prp.name} |'\n",
    "    f' {prp.total_memory / 1e9} GB Memory')\n",
    "\n",
    "data_path = 'data/PGT-DATA.txt'\n",
    "dataset = DatasetPGT(batch_size=batch)\n",
    "\n",
    "Config = get_config_by_name('PGT-As', dataset.vocab_size)\n",
    "Config.load = True\n",
    "\n",
    "Config.data_path = data_path\n",
    "dataset.chunk = Config.chunk\n",
    "\n",
    "data = open(Config.data_path, 'r', encoding=\"utf8\").read()\n",
    "tvl = len(data)\n",
    "use_tvl = tvl * percentage\n",
    "print(f'TOTAL DATA : {tvl}')\n",
    "print(f'SELECTED DATA : {int(use_tvl)}')\n",
    "selected_data = data[:int(use_tvl)]\n",
    "dataset.src = selected_data\n",
    "with open('selected.txt', 'w', encoding='utf8') as wr:\n",
    "    wr.write(selected_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "Config.batch_size = batch\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=Config.batch_size, num_workers=4,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "if Config.load:\n",
    "    fprint('Loading Model ...')\n",
    "    model = PGT(config=Config).to('cpu')\n",
    "    loaded = torch.load('model.pt', 'cpu')\n",
    "    model.load_state_dict(loaded['model'])\n",
    "    model = model.to(Config.device)\n",
    "    fprint(f'Model Loaded With {sum(p.numel() for p in model.parameters()) / 1e6} Million Parameters')\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = model.configure_optimizer(Config)\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), Config.lr)\n",
    "    # optimizer = model.configure_optimizer(Config)\n",
    "    optimizer.load_state_dict(loaded['optimizer'])\n",
    "else:\n",
    "    fprint('Creating Model ...')\n",
    "    model = PGT(config=Config).to('cpu').to(Config.device)\n",
    "    fprint(f'Model Created With {sum(p.numel() for p in model.parameters()) / 1e6} Million Parameters')\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = model.configure_optimizer(Config)\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), Config.lr)\n",
    "model = torch.compile(model)\n",
    "\n",
    "del data,\n",
    "total_iterations = dataset.__len__() // Config.batch_size\n",
    "question = dataset.encode('what do you know about dubai').to(Config.device)\n",
    "question = question['input_ids'].to(Config.device)\n",
    "mxl = math.ceil(dataset.__len__() / Config.batch_size)\n",
    "print('TRAINING IS ABOUT TO START')\n",
    "if Config.load:\n",
    "    for epoch in range(loaded['epoch'], Config.epochs):\n",
    "        loss_avg = 0\n",
    "        st = time.time()\n",
    "        for i, (inp, label) in enumerate(dataloader):\n",
    "            inp = inp.type(torch.long)\n",
    "            label = label.type(torch.long)\n",
    "            inp = make2d(inp).to(Config.device)\n",
    "            label = make2d(label).to(Config.device)\n",
    "            predict = model(inputs=inp)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss = criterion(predict.permute(0, 2, 1), label.view(-1, label.size(-1)))\n",
    "            loss_avg += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            fprint(\n",
    "                f'\\rEPOCH : [{epoch + 1}/{Config.epochs}] | LOSS : {loss.item() / Config.batch_size} | EPOCH LOSS AVG : {(loss_avg / (i + 1)) / Config.batch_size} | ITER : {i + 1}/{mxl} | DEVICE : {Config.device} | EPOCH TIME {int(time.time() - st)} SEC',\n",
    "                end='')\n",
    "\n",
    "        print()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print()\n",
    "            save_model(model=model.state_dict(), optimizer=optimizer.state_dict(), epochs=Config.epochs,\n",
    "                       epoch=epoch + 1,\n",
    "                       name='modified_model.pt')\n",
    "            fprint('==> MODEL SAVED SUCCESSFULLY')\n",
    "            predictions = model.generate(idx=question, eos=dataset.tokenizer.eos_token_id,\n",
    "                                         generate=256\n",
    "\n",
    "                                         )\n",
    "            fprint(f'QUESTION : {dataset.decode(question)}')\n",
    "            fprint(f'PREDICTION : {dataset.decode(predictions)}')\n",
    "else:\n",
    "    for epoch in range(Config.epochs):\n",
    "        loss_avg = 0\n",
    "        st = time.time()\n",
    "        for i, (inp, label) in enumerate(dataloader):\n",
    "            inp = make2d(inp).to(Config.device)\n",
    "            label = make2d(label).to(Config.device)\n",
    "            predict = model(inputs=inp)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss = criterion(predict.permute(0, 2, 1), label.view(-1, label.size(-1)))\n",
    "            loss_avg += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            fprint(\n",
    "                f'\\rEPOCH : [{epoch + 1}/{Config.epochs}] | LOSS : {loss.item() / Config.batch_size} | EPOCH LOSS AVG : {(loss_avg / (i + 1)) / Config.batch_size} | ITER : {i + 1}/{mxl} | DEVICE : {Config.device} | EPOCH TIME {int(time.time() - st)} SEC',\n",
    "                end='')\n",
    "\n",
    "        print()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print()\n",
    "            save_model(model=model.state_dict(), optimizer=optimizer.state_dict(), epochs=Config.epochs,\n",
    "                       epoch=epoch + 1,\n",
    "                       name='model.pt')\n",
    "            fprint('==> MODEL SAVED SUCCESSFULLY')\n",
    "            predictions = model.generate(idx=question, eos=dataset.tokenizer.eos_token_id,\n",
    "                                         generate=256\n",
    "\n",
    "                                         )\n",
    "            fprint(f'QUESTION : {dataset.decode(question)}')\n",
    "            fprint(f'PREDICTION : {dataset.decode(predictions)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}